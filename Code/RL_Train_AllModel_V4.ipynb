{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jerry Jiang\n",
    "\n",
    "# V4 reward: Entropy Penalty\n",
    "# Reward = base (+1/-0.2) - 0.05 Ã— entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsj31\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variable\n",
    "Version = \"V4\"\n",
    "bert_model_path = \"../Model/sentiment_bert\"\n",
    "train_data_path = \"../Dataset/train_preprocessed.csv\"\n",
    "supervised_model_path = \"../Model/policy_net_supervised.pt\"\n",
    "save_model_path = f\"../Model/{Version}\"\n",
    "logs_path = f\"../Logs/{Version}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using BERT model from: ../Model/sentiment_bert\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.3, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using BERT model from: {bert_model_path}\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(str(bert_model_path), local_files_only=True)\n",
    "config = BertConfig.from_pretrained(str(bert_model_path), output_hidden_states=True, local_files_only=True)\n",
    "bert = BertModel.from_pretrained(str(bert_model_path), config=config, local_files_only=True).to(device)\n",
    "bert.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(train_data_path)\n",
    "texts = train_data[\"Phrase\"].astype(str).tolist()\n",
    "labels = train_data[\"Sentiment\"].tolist()\n",
    "\n",
    "encodings = tokenizer(\n",
    "    texts,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=128,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "train_dataset = SentimentDataset(encodings, labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy (Actor) network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim=768, hidden_dim=128, output_dim=5):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)  # logits\n",
    "\n",
    "# Value (Critic) network\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_dim=768, hidden_dim=128):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsj31\\AppData\\Local\\Temp\\ipykernel_21612\\3558846925.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  policy_net.load_state_dict(torch.load(supervised_model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Before RL] Accuracy: 0.7262 | CrossEntropy Loss: 0.6818\n"
     ]
    }
   ],
   "source": [
    "# === Step 1: Initialize policy network from supervised model ===\n",
    "policy_net = PolicyNetwork().to(device)\n",
    "policy_net.load_state_dict(torch.load(supervised_model_path))\n",
    "policy_net.train()\n",
    "\n",
    "# === Step 2: Evaluate initial accuracy and loss before RL training ===\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "policy_net.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "total_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        logits = policy_net(cls_embeddings)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "loss_before = total_loss / len(train_loader)\n",
    "acc_before = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "print(f\"[Before RL] Accuracy: {acc_before:.4f} | CrossEntropy Loss: {loss_before:.4f}\")\n",
    "\n",
    "policy_net.train()\n",
    "\n",
    "# === Step 3: Initialize value network and optimizers ===\n",
    "value_net = ValueNetwork().to(device)\n",
    "actor_optimizer = optim.Adam(policy_net.parameters(), lr=1e-5)\n",
    "critic_optimizer = optim.Adam(value_net.parameters(), lr=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V4 reward: Entropy Penalty\n",
    "# Reward = base (+1/-0.2) - 0.05 Ã— entropy\n",
    "def compute_reward(preds, labels):\n",
    "    pred_labels = torch.argmax(preds, dim=1)\n",
    "    correct = (pred_labels == labels).float()\n",
    "    probs = torch.softmax(preds, dim=1)\n",
    "    entropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=1)\n",
    "    reward = correct * 1.0 + (1 - correct) * -0.2\n",
    "    reward -= 0.05 * entropy\n",
    "    return reward\n",
    "\n",
    "def compute_entropy(logits):\n",
    "    prob = torch.softmax(logits, dim=1)\n",
    "    entropy = -torch.sum(prob * torch.log(prob + 1e-8), dim=1)\n",
    "    return entropy.mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. A2C Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5853 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5853/5853 [05:10<00:00, 18.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Loss: 1627.3028 | Reward: 0.6121 | Accuracy: 0.7162 | Entropy: 0.9460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5853/5853 [05:11<00:00, 18.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Loss: 1580.3023 | Reward: 0.3036 | Accuracy: 0.4827 | Entropy: 1.5109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5853/5853 [05:12<00:00, 18.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Loss: 1163.1340 | Reward: -0.0341 | Accuracy: 0.2045 | Entropy: 1.5890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5853/5853 [05:12<00:00, 18.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Loss: 959.2196 | Reward: -0.0833 | Accuracy: 0.1637 | Entropy: 1.5960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5853/5853 [05:22<00:00, 18.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Loss: 717.3697 | Reward: -0.1379 | Accuracy: 0.1184 | Entropy: 1.5988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5853/5853 [05:32<00:00, 17.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Loss: 933.7834 | Reward: -0.0867 | Accuracy: 0.1612 | Entropy: 1.6013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5853/5853 [05:22<00:00, 18.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Loss: 1012.1348 | Reward: -0.0669 | Accuracy: 0.1777 | Entropy: 1.6018\n",
      "Saved A2C policy model to: ../Model/V4\\policy_net_rl_a2c_V4.pt\n",
      "Saved A2C value model to: ../Model/V4\\value_net_rl_a2c_V4.pt\n",
      "Saved A2C logs to: ../Logs/V4\\a2c_V4.json\n",
      "[Comparison to Supervised]\n",
      "Accuracy Before: 0.7262 | After: 0.1777 | Î”: -0.5485 (-75.54%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 7\n",
    "train_logs = {\n",
    "    \"loss\": [],\n",
    "    \"reward\": [],\n",
    "    \"accuracy\": [],\n",
    "    \"entropy\": []\n",
    "}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    total_reward = 0\n",
    "    total_entropy = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            cls_embeddings = output.last_hidden_state[:, 0, :]\n",
    "\n",
    "        # ---- Actor forward\n",
    "        logits = policy_net(cls_embeddings)\n",
    "        log_probs = torch.log_softmax(logits, dim=1)\n",
    "        probs = torch.exp(log_probs)\n",
    "        sampled_action = torch.multinomial(probs, num_samples=1).squeeze()\n",
    "        log_prob = log_probs[range(len(sampled_action)), sampled_action]\n",
    "\n",
    "        # ---- Critic forward\n",
    "        value = value_net(cls_embeddings)  # [B]\n",
    "        reward = compute_reward(logits, labels)\n",
    "        advantage = reward - value.detach()\n",
    "\n",
    "        # ---- Losses\n",
    "        policy_loss = - (log_prob * advantage).mean()\n",
    "        value_loss = F.mse_loss(value, reward)\n",
    "        total_batch_loss = policy_loss + value_loss\n",
    "\n",
    "        # ---- Accuracy and entropy\n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "        correct += (pred == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        entropy = compute_entropy(logits)\n",
    "\n",
    "        # ---- Backprop\n",
    "        actor_optimizer.zero_grad()\n",
    "        critic_optimizer.zero_grad()\n",
    "        total_batch_loss.backward()\n",
    "        actor_optimizer.step()\n",
    "        critic_optimizer.step()\n",
    "\n",
    "        total_loss += total_batch_loss.item()\n",
    "        total_reward += reward.mean().item()\n",
    "        total_entropy += entropy\n",
    "\n",
    "    epoch_acc = correct / total\n",
    "    epoch_loss = total_loss\n",
    "    epoch_reward = total_reward / len(train_loader)\n",
    "    epoch_entropy = total_entropy / len(train_loader)\n",
    "\n",
    "    train_logs[\"loss\"].append(epoch_loss)\n",
    "    train_logs[\"reward\"].append(epoch_reward)\n",
    "    train_logs[\"accuracy\"].append(epoch_acc)\n",
    "    train_logs[\"entropy\"].append(epoch_entropy)\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}] Loss: {epoch_loss:.4f} | Reward: {epoch_reward:.4f} | Accuracy: {epoch_acc:.4f} | Entropy: {epoch_entropy:.4f}\")\n",
    "\n",
    "# Save A2C model and value\n",
    "torch.save(policy_net.state_dict(), os.path.join(save_model_path, \"policy_net_rl_a2c_\" + Version + \".pt\"))\n",
    "torch.save(value_net.state_dict(), os.path.join(save_model_path, \"value_net_rl_a2c_\" + Version + \".pt\"))\n",
    "\n",
    "with open(os.path.join(logs_path, \"a2c_\" + Version + \".json\"), \"w\") as f:\n",
    "    json.dump(train_logs, f, indent=2)\n",
    "\n",
    "print(\"Saved A2C policy model to:\", os.path.join(save_model_path, \"policy_net_rl_a2c_\" + Version + \".pt\"))\n",
    "print(\"Saved A2C value model to:\", os.path.join(save_model_path, \"value_net_rl_a2c_\" + Version + \".pt\"))\n",
    "print(\"Saved A2C logs to:\", os.path.join(logs_path, \"a2c_\" + Version + \".json\"))\n",
    "\n",
    "# compare final result\n",
    "acc_after = train_logs[\"accuracy\"][-1]\n",
    "acc_change = acc_after - acc_before\n",
    "acc_pct = (acc_change / acc_before) * 100 if acc_before > 0 else 0\n",
    "\n",
    "print(f\"[Comparison to Supervised]\")\n",
    "print(f\"Accuracy Before: {acc_before:.4f} | After: {acc_after:.4f} | Î”: {acc_change:+.4f} ({acc_pct:+.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. REINFORCE Begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsj31\\AppData\\Local\\Temp\\ipykernel_21612\\4181386562.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  policy_net.load_state_dict(torch.load(supervised_model_path))\n"
     ]
    }
   ],
   "source": [
    "# REINFORCE\n",
    "policy_net = PolicyNetwork().to(device)\n",
    "policy_net.load_state_dict(torch.load(supervised_model_path))\n",
    "policy_net.train()\n",
    "\n",
    "value_net = None  # REINFORCE does not use value network\n",
    "actor_optimizer = torch.optim.Adam(policy_net.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "REINFORCE Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5853/5853 [05:15<00:00, 18.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REINFORCE][Epoch 1] Loss: 1807.8255 | Reward: 0.6187 | Acc: 0.7071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "REINFORCE Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5853/5853 [05:14<00:00, 18.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REINFORCE][Epoch 2] Loss: 1924.5426 | Reward: 0.6114 | Acc: 0.7021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "REINFORCE Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5853/5853 [05:18<00:00, 18.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REINFORCE][Epoch 3] Loss: 2126.1112 | Reward: 0.5920 | Acc: 0.6886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "REINFORCE Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5853/5853 [05:18<00:00, 18.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REINFORCE][Epoch 4] Loss: 2306.7905 | Reward: 0.5338 | Acc: 0.6440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "REINFORCE Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5853/5853 [05:15<00:00, 18.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REINFORCE][Epoch 5] Loss: 2311.5131 | Reward: 0.5710 | Acc: 0.6726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "REINFORCE Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5853/5853 [05:15<00:00, 18.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REINFORCE][Epoch 6] Loss: 2132.2645 | Reward: 0.6023 | Acc: 0.6964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "REINFORCE Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5853/5853 [05:13<00:00, 18.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REINFORCE][Epoch 7] Loss: 1893.6564 | Reward: 0.6043 | Acc: 0.6953\n",
      "Saved REINFORCE policy model to: ../Model/V4\\policy_net_rl_reinforce_V4.pt\n",
      "Saved REINFORCE logs to: ../Logs/V4\\reinforce_V4.json\n",
      "[Comparison to Supervised]\n",
      "Accuracy Before: 0.7262 | After: 0.6953 | Î”: -0.0309 (-4.25%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_logs = {\"loss\": [], \"reward\": [], \"accuracy\": [], \"entropy\": []}\n",
    "epochs = 7\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss, total_reward, total_entropy, correct, total = 0, 0, 0, 0, 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"REINFORCE Epoch {epoch+1}\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            cls_embeds = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        logits = policy_net(cls_embeds)\n",
    "        log_probs = torch.log_softmax(logits, dim=1)\n",
    "        probs = torch.exp(log_probs)\n",
    "        sampled_action = torch.multinomial(probs, num_samples=1).squeeze()\n",
    "        log_prob = log_probs[range(len(sampled_action)), sampled_action]\n",
    "\n",
    "        reward = compute_reward(logits, labels)\n",
    "        entropy = compute_entropy(logits)\n",
    "\n",
    "        loss = - (log_prob * reward.detach()).mean()\n",
    "        actor_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        actor_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_reward += reward.mean().item()\n",
    "        total_entropy += entropy\n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "        correct += (pred == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    train_logs[\"loss\"].append(total_loss)\n",
    "    train_logs[\"reward\"].append(total_reward / len(train_loader))\n",
    "    train_logs[\"accuracy\"].append(acc)\n",
    "    train_logs[\"entropy\"].append(total_entropy / len(train_loader))\n",
    "\n",
    "    print(f\"[REINFORCE][Epoch {epoch+1}] Loss: {total_loss:.4f} | Reward: {train_logs['reward'][-1]:.4f} | Acc: {acc:.4f}\")\n",
    "\n",
    "# Save REINFORCE policy only\n",
    "torch.save(policy_net.state_dict(), os.path.join(save_model_path, \"policy_net_rl_reinforce_\" + Version + \".pt\"))\n",
    "\n",
    "with open(os.path.join(logs_path, \"reinforce_\" + Version + \".json\"), \"w\") as f:\n",
    "    json.dump(train_logs, f, indent=2)\n",
    "\n",
    "print(\"Saved REINFORCE policy model to:\", os.path.join(save_model_path, \"policy_net_rl_reinforce_\" + Version + \".pt\"))\n",
    "print(\"Saved REINFORCE logs to:\", os.path.join(logs_path, \"reinforce_\" + Version + \".json\"))\n",
    "\n",
    "# compare final result\n",
    "acc_after = train_logs[\"accuracy\"][-1]\n",
    "acc_change = acc_after - acc_before\n",
    "acc_pct = (acc_change / acc_before) * 100 if acc_before > 0 else 0\n",
    "\n",
    "print(f\"[Comparison to Supervised]\")\n",
    "print(f\"Accuracy Before: {acc_before:.4f} | After: {acc_after:.4f} | Î”: {acc_change:+.4f} ({acc_pct:+.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. REINFORCE_Baseline Begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsj31\\AppData\\Local\\Temp\\ipykernel_21612\\2010653356.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  policy_net.load_state_dict(torch.load(supervised_model_path))\n"
     ]
    }
   ],
   "source": [
    "# REINFORCE_Baseline\n",
    "policy_net = PolicyNetwork().to(device)\n",
    "policy_net.load_state_dict(torch.load(supervised_model_path))\n",
    "policy_net.train()\n",
    "\n",
    "value_net = ValueNetwork().to(device)\n",
    "actor_optimizer = torch.optim.Adam(policy_net.parameters(), lr=2e-5)\n",
    "critic_optimizer = torch.optim.Adam(value_net.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "REINFORCE_Baseline Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5853/5853 [05:20<00:00, 18.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REINFORCE_Baseline][Epoch 1] Loss: 1620.9172 | Reward: 0.4055 | Acc: 0.5559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "REINFORCE_Baseline Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5853/5853 [05:26<00:00, 17.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REINFORCE_Baseline][Epoch 2] Loss: 1277.2256 | Reward: 0.0333 | Acc: 0.2605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "REINFORCE_Baseline Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5853/5853 [05:31<00:00, 17.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REINFORCE_Baseline][Epoch 3] Loss: 962.1332 | Reward: -0.0586 | Acc: 0.1845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "REINFORCE_Baseline Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5853/5853 [05:32<00:00, 17.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REINFORCE_Baseline][Epoch 4] Loss: 844.8913 | Reward: -0.1060 | Acc: 0.1450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "REINFORCE_Baseline Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5853/5853 [05:30<00:00, 17.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REINFORCE_Baseline][Epoch 5] Loss: 465.8679 | Reward: -0.1977 | Acc: 0.0688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "REINFORCE_Baseline Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5853/5853 [06:05<00:00, 16.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REINFORCE_Baseline][Epoch 6] Loss: 409.8988 | Reward: -0.2081 | Acc: 0.0601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "REINFORCE_Baseline Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5853/5853 [05:31<00:00, 17.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REINFORCE_Baseline][Epoch 7] Loss: 1060.5509 | Reward: -0.0353 | Acc: 0.2041\n",
      "Saved REINFORCE_Baseline policy model to: ../Model/V4\\policy_net_rl_reinforce_baseline_V4.pt\n",
      "Saved REINFORCE_Baseline value model to: ../Model/V4\\value_net_rl_reinforce_baseline_V4.pt\n",
      "Saved REINFORCE_Baseline logs to: ../Logs/V4\\reinforce_baseline_V4.json\n",
      "[Comparison to Supervised]\n",
      "Accuracy Before: 0.7262 | After: 0.2041 | Î”: -0.5221 (-71.89%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_logs = {\"loss\": [], \"reward\": [], \"accuracy\": [], \"entropy\": []}\n",
    "epochs = 7\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss, total_reward, total_entropy, correct, total = 0, 0, 0, 0, 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"REINFORCE_Baseline Epoch {epoch+1}\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            cls_embeds = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        logits = policy_net(cls_embeds)\n",
    "        log_probs = torch.log_softmax(logits, dim=1)\n",
    "        probs = torch.exp(log_probs)\n",
    "        sampled_action = torch.multinomial(probs, num_samples=1).squeeze()\n",
    "        log_prob = log_probs[range(len(sampled_action)), sampled_action]\n",
    "\n",
    "        reward = compute_reward(logits, labels)\n",
    "        entropy = compute_entropy(logits)\n",
    "\n",
    "        value = value_net(cls_embeds)\n",
    "        advantage = reward - value.detach()\n",
    "\n",
    "        policy_loss = - (log_prob * advantage).mean()\n",
    "        value_loss = F.mse_loss(value, reward)\n",
    "        loss = policy_loss + value_loss\n",
    "\n",
    "        actor_optimizer.zero_grad()\n",
    "        critic_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        actor_optimizer.step()\n",
    "        critic_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_reward += reward.mean().item()\n",
    "        total_entropy += entropy\n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "        correct += (pred == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    train_logs[\"loss\"].append(total_loss)\n",
    "    train_logs[\"reward\"].append(total_reward / len(train_loader))\n",
    "    train_logs[\"accuracy\"].append(acc)\n",
    "    train_logs[\"entropy\"].append(total_entropy / len(train_loader))\n",
    "\n",
    "    print(f\"[REINFORCE_Baseline][Epoch {epoch+1}] Loss: {total_loss:.4f} | Reward: {train_logs['reward'][-1]:.4f} | Acc: {acc:.4f}\")\n",
    "\n",
    "# Save REINFORCE_Baseline policy and value\n",
    "torch.save(policy_net.state_dict(), os.path.join(save_model_path, \"policy_net_rl_reinforce_baseline_\" + Version + \".pt\"))\n",
    "torch.save(value_net.state_dict(), os.path.join(save_model_path, \"value_net_rl_reinforce_baseline_\" + Version + \".pt\"))\n",
    "\n",
    "with open(os.path.join(logs_path, \"reinforce_baseline_\" + Version + \".json\"), \"w\") as f:\n",
    "    json.dump(train_logs, f, indent=2)\n",
    "\n",
    "print(\"Saved REINFORCE_Baseline policy model to:\", os.path.join(save_model_path, \"policy_net_rl_reinforce_baseline_\" + Version + \".pt\"))\n",
    "print(\"Saved REINFORCE_Baseline value model to:\", os.path.join(save_model_path, \"value_net_rl_reinforce_baseline_\" + Version + \".pt\"))\n",
    "print(\"Saved REINFORCE_Baseline logs to:\", os.path.join(logs_path, \"reinforce_baseline_\" + Version + \".json\"))\n",
    "\n",
    "# compare final result\n",
    "acc_after = train_logs[\"accuracy\"][-1]\n",
    "acc_change = acc_after - acc_before\n",
    "acc_pct = (acc_change / acc_before) * 100 if acc_before > 0 else 0\n",
    "\n",
    "print(f\"[Comparison to Supervised]\")\n",
    "print(f\"Accuracy Before: {acc_before:.4f} | After: {acc_after:.4f} | Î”: {acc_change:+.4f} ({acc_pct:+.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. SCST Begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsj31\\AppData\\Local\\Temp\\ipykernel_21612\\475260602.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  policy_net.load_state_dict(torch.load(supervised_model_path))\n"
     ]
    }
   ],
   "source": [
    "# SCST\n",
    "policy_net = PolicyNetwork().to(device)\n",
    "policy_net.load_state_dict(torch.load(supervised_model_path))\n",
    "policy_net.train()\n",
    "\n",
    "value_net = ValueNetwork().to(device)\n",
    "actor_optimizer = torch.optim.Adam(policy_net.parameters(), lr=2e-5)\n",
    "critic_optimizer = torch.optim.Adam(value_net.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SCST Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5853/5853 [06:17<00:00, 15.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCST][Epoch 1] Loss: 1537.8501 | Reward: 0.3823 | Acc: 0.5364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SCST Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5853/5853 [06:05<00:00, 16.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCST][Epoch 2] Loss: 817.1373 | Reward: -0.1134 | Acc: 0.1384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SCST Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5853/5853 [05:22<00:00, 18.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCST][Epoch 3] Loss: 836.1066 | Reward: -0.1016 | Acc: 0.1485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SCST Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5853/5853 [05:20<00:00, 18.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCST][Epoch 4] Loss: 978.0521 | Reward: -0.0613 | Acc: 0.1821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SCST Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5853/5853 [05:20<00:00, 18.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCST][Epoch 5] Loss: 715.8979 | Reward: -0.1309 | Acc: 0.1243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SCST Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5853/5853 [05:20<00:00, 18.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCST][Epoch 6] Loss: 919.1889 | Reward: -0.0762 | Acc: 0.1700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SCST Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5853/5853 [05:22<00:00, 18.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SCST][Epoch 7] Loss: 1270.0165 | Reward: 0.1277 | Acc: 0.3399\n",
      "Saved SCST policy model to: ../Model/V4\\policy_net_rl_scst_V4.pt\n",
      "Saved SCST value model to: ../Model/V4\\value_net_rl_scst_V4.pt\n",
      "Saved SCST logs to: ../Logs/V4\\scst_V4.json\n",
      "[Comparison to Supervised]\n",
      "Accuracy Before: 0.7262 | After: 0.3399 | Î”: -0.3863 (-53.20%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_logs = {\"loss\": [], \"reward\": [], \"accuracy\": [], \"entropy\": []}\n",
    "epochs = 7\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss, total_reward, total_entropy, correct, total = 0, 0, 0, 0, 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"SCST Epoch {epoch+1}\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            cls_embeds = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        logits = policy_net(cls_embeds)\n",
    "        log_probs = torch.log_softmax(logits, dim=1)\n",
    "        probs = torch.exp(log_probs)\n",
    "        sampled_action = torch.multinomial(probs, num_samples=1).squeeze()\n",
    "        log_prob = log_probs[range(len(sampled_action)), sampled_action]\n",
    "\n",
    "        reward = compute_reward(logits, labels)\n",
    "        entropy = compute_entropy(logits)\n",
    "\n",
    "        value = value_net(cls_embeds)\n",
    "        advantage = reward - value.detach()\n",
    "\n",
    "        policy_loss = - (log_prob * advantage).mean()\n",
    "        value_loss = F.mse_loss(value, reward)\n",
    "        loss = policy_loss + value_loss\n",
    "\n",
    "        actor_optimizer.zero_grad()\n",
    "        critic_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        actor_optimizer.step()\n",
    "        critic_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_reward += reward.mean().item()\n",
    "        total_entropy += entropy\n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "        correct += (pred == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    train_logs[\"loss\"].append(total_loss)\n",
    "    train_logs[\"reward\"].append(total_reward / len(train_loader))\n",
    "    train_logs[\"accuracy\"].append(acc)\n",
    "    train_logs[\"entropy\"].append(total_entropy / len(train_loader))\n",
    "\n",
    "    print(f\"[SCST][Epoch {epoch+1}] Loss: {total_loss:.4f} | Reward: {train_logs['reward'][-1]:.4f} | Acc: {acc:.4f}\")\n",
    "\n",
    "# save SCST policy and value\n",
    "torch.save(policy_net.state_dict(), os.path.join(save_model_path, \"policy_net_rl_scst_\" + Version + \".pt\"))\n",
    "torch.save(value_net.state_dict(), os.path.join(save_model_path, \"value_net_rl_scst_\" + Version + \".pt\"))\n",
    "\n",
    "with open(os.path.join(logs_path, \"scst_\" + Version + \".json\"), \"w\") as f:\n",
    "    json.dump(train_logs, f, indent=2)\n",
    "\n",
    "print(\"Saved SCST policy model to:\", os.path.join(save_model_path, \"policy_net_rl_scst_\" + Version + \".pt\"))\n",
    "print(\"Saved SCST value model to:\", os.path.join(save_model_path, \"value_net_rl_scst_\" + Version + \".pt\"))\n",
    "print(\"Saved SCST logs to:\", os.path.join(logs_path, \"scst_\" + Version + \".json\"))\n",
    "\n",
    "# compare final result\n",
    "acc_after = train_logs[\"accuracy\"][-1]\n",
    "acc_change = acc_after - acc_before\n",
    "acc_pct = (acc_change / acc_before) * 100 if acc_before > 0 else 0\n",
    "\n",
    "print(f\"[Comparison to Supervised]\")\n",
    "print(f\"Accuracy Before: {acc_before:.4f} | After: {acc_after:.4f} | Î”: {acc_change:+.4f} ({acc_pct:+.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. PPO Begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsj31\\AppData\\Local\\Temp\\ipykernel_21612\\2958709668.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  policy_net.load_state_dict(torch.load(supervised_model_path))\n"
     ]
    }
   ],
   "source": [
    "# PPO\n",
    "policy_net = PolicyNetwork().to(device)\n",
    "policy_net.load_state_dict(torch.load(supervised_model_path))\n",
    "policy_net.train()\n",
    "\n",
    "value_net = ValueNetwork().to(device)\n",
    "actor_optimizer = torch.optim.Adam(policy_net.parameters(), lr=2e-5)\n",
    "critic_optimizer = torch.optim.Adam(value_net.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPO Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5853/5853 [05:22<00:00, 18.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PPO][Epoch 1] Loss: 1518.3811 | Reward: 0.6235 | Acc: 0.7024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPO Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5853/5853 [05:21<00:00, 18.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PPO][Epoch 2] Loss: 1493.1971 | Reward: 0.5624 | Acc: 0.6425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPO Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5853/5853 [05:23<00:00, 18.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PPO][Epoch 3] Loss: 1389.0136 | Reward: 0.4977 | Acc: 0.5853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPO Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5853/5853 [05:29<00:00, 17.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PPO][Epoch 4] Loss: 1306.5856 | Reward: 0.4682 | Acc: 0.5588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPO Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5853/5853 [05:34<00:00, 17.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PPO][Epoch 5] Loss: 1260.9681 | Reward: 0.4554 | Acc: 0.5475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPO Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5853/5853 [05:34<00:00, 17.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PPO][Epoch 6] Loss: 1197.3944 | Reward: 0.4370 | Acc: 0.5317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPO Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5853/5853 [05:31<00:00, 17.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PPO][Epoch 7] Loss: 1160.1699 | Reward: 0.4234 | Acc: 0.5200\n",
      "Saved PPO policy model to: ../Model/V4\\policy_net_rl_ppo_V4.pt\n",
      "Saved PPO value model to: ../Model/V4\\value_net_rl_ppo_V4.pt\n",
      "Saved PPO logs to: ../Logs/V4\\ppo_V4.json\n",
      "[Comparison to Supervised]\n",
      "Accuracy Before: 0.7262 | After: 0.5200 | Î”: -0.2062 (-28.39%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_logs = {\"loss\": [], \"reward\": [], \"accuracy\": [], \"entropy\": []}\n",
    "epochs = 7\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss, total_reward, total_entropy, correct, total = 0, 0, 0, 0, 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"PPO Epoch {epoch+1}\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            cls_embeds = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        logits = policy_net(cls_embeds)\n",
    "        log_probs = torch.log_softmax(logits, dim=1)\n",
    "        probs = torch.exp(log_probs)\n",
    "        sampled_action = torch.multinomial(probs, num_samples=1).squeeze()\n",
    "        log_prob = log_probs[range(len(sampled_action)), sampled_action]\n",
    "\n",
    "        reward = compute_reward(logits, labels)\n",
    "        entropy = compute_entropy(logits)\n",
    "\n",
    "        value = value_net(cls_embeds)\n",
    "        advantage = reward - value.detach()\n",
    "\n",
    "        old_log_prob = log_prob.detach()\n",
    "        new_logits = policy_net(cls_embeds)\n",
    "        new_log_probs = torch.log_softmax(new_logits, dim=1)\n",
    "        new_log_prob = new_log_probs[range(len(sampled_action)), sampled_action]\n",
    "\n",
    "        ratio = torch.exp(new_log_prob - old_log_prob)\n",
    "        surr1 = ratio * advantage\n",
    "        surr2 = torch.clamp(ratio, 0.8, 1.2) * advantage\n",
    "\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "        value_loss = F.mse_loss(value, reward)\n",
    "        loss = policy_loss + value_loss\n",
    "\n",
    "        actor_optimizer.zero_grad()\n",
    "        critic_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        actor_optimizer.step()\n",
    "        critic_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_reward += reward.mean().item()\n",
    "        total_entropy += entropy\n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "        correct += (pred == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    acc = correct / total\n",
    "    train_logs[\"loss\"].append(total_loss)\n",
    "    train_logs[\"reward\"].append(total_reward / len(train_loader))\n",
    "    train_logs[\"accuracy\"].append(acc)\n",
    "    train_logs[\"entropy\"].append(total_entropy / len(train_loader))\n",
    "\n",
    "    print(f\"[PPO][Epoch {epoch+1}] Loss: {total_loss:.4f} | Reward: {train_logs['reward'][-1]:.4f} | Acc: {acc:.4f}\")\n",
    "\n",
    "# Save PPO policy and value\n",
    "torch.save(policy_net.state_dict(), os.path.join(save_model_path, \"policy_net_rl_ppo_\" + Version + \".pt\"))\n",
    "torch.save(value_net.state_dict(), os.path.join(save_model_path, \"value_net_rl_ppo_\" + Version + \".pt\"))\n",
    "\n",
    "with open(os.path.join(logs_path, \"ppo_\" + Version + \".json\"), \"w\") as f:\n",
    "    json.dump(train_logs, f, indent=2)\n",
    "\n",
    "print(\"Saved PPO policy model to:\", os.path.join(save_model_path, \"policy_net_rl_ppo_\" + Version + \".pt\"))\n",
    "print(\"Saved PPO value model to:\", os.path.join(save_model_path, \"value_net_rl_ppo_\" + Version + \".pt\"))\n",
    "print(\"Saved PPO logs to:\", os.path.join(logs_path, \"ppo_\" + Version + \".json\"))\n",
    "\n",
    "# compare final result\n",
    "acc_after = train_logs[\"accuracy\"][-1]\n",
    "acc_change = acc_after - acc_before\n",
    "acc_pct = (acc_change / acc_before) * 100 if acc_before > 0 else 0\n",
    "\n",
    "print(f\"[Comparison to Supervised]\")\n",
    "print(f\"Accuracy Before: {acc_before:.4f} | After: {acc_after:.4f} | Î”: {acc_change:+.4f} ({acc_pct:+.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
